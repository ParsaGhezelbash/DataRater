{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b65031-f963-4da4-96b2-37cfa833dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import numpy as np\n",
    "from scipy import stats # Import the stats module from SciPy\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "from models import construct_model\n",
    "from datasets import MNISTDataRaterDataset, DataCorruptionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8668a44d-27ee-4e07-b391-c9cdb9954666",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d1cb7aa-94ed-4269-bb84-72cce7215d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Config dataclass\n",
    "# -------------------------\n",
    "@dataclass\n",
    "class DownstreamConfig:\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size: int = 128\n",
    "    train_split_ratio: float = 0.8\n",
    "    epochs: int = 5\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 0.0\n",
    "    keep_threshold: float = 0.75   # (unused now, kept for back-compat)\n",
    "    seed: int = 42\n",
    "    drop_frac: float = 0.01        # fraction to drop per batch for filtered/random\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, data_loader, device) -> float:\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x, y in data_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / max(1, total)\n",
    "\n",
    "def train_one_epoch_baseline(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    running_loss = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * y.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "def train_one_epoch_filtered(\n",
    "    model,\n",
    "    data_rater,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    drop_frac: float = 0.25,   # drop bottom 25% of each batch\n",
    "    center: bool = False,      # optional: center scores before ranking\n",
    "    min_keep: int = 1          # always keep at least this many\n",
    "):\n",
    "    \"\"\"\n",
    "    Train using only the top (1 - drop_frac) fraction by DataRater score per batch.\n",
    "    Uses raw scores (no softmax), so it's not sensitive to batch size.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    data_rater.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    drop_frac = float(drop_frac)\n",
    "    drop_frac = max(0.0, min(0.99, drop_frac))  # clamp for sanity\n",
    "    min_keep = max(1, int(min_keep))\n",
    "\n",
    "    kept_total = 0\n",
    "    seen_total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        B = y.size(0)\n",
    "        seen_total += B\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = data_rater(x).squeeze(-1)  # [B]\n",
    "            if center:\n",
    "                scores = scores - scores.mean()\n",
    "\n",
    "        # how many to keep this batch\n",
    "        keep_k = max(min_keep, B - int(B * drop_frac))\n",
    "        keep_k = min(keep_k, B)  # safety\n",
    "\n",
    "        # take top-k by raw score\n",
    "        _, top_idx = torch.topk(scores, k=keep_k, largest=True, sorted=False)\n",
    "        keep_mask = torch.zeros(B, dtype=torch.bool, device=x.device)\n",
    "        keep_mask[top_idx] = True\n",
    "\n",
    "        x_keep = x[keep_mask]\n",
    "        y_keep = y[keep_mask]\n",
    "        kept_total += y_keep.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_keep)\n",
    "        loss = loss_fn(logits, y_keep)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * y_keep.size(0)\n",
    "\n",
    "    avg_loss = running_loss / max(1, kept_total)\n",
    "    acceptance_rate = kept_total / max(1, seen_total)\n",
    "    return avg_loss, acceptance_rate\n",
    "\n",
    "def train_one_epoch_random_drop(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    drop_frac: float = 0.25,  # drop bottom 25% at random (no rater)\n",
    "    min_keep: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Train keeping a random top (1 - drop_frac) fraction per batch.\n",
    "    Provides a control to compare against DataRater-based filtering.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    drop_frac = float(drop_frac)\n",
    "    drop_frac = max(0.0, min(0.99, drop_frac))\n",
    "    min_keep = max(1, int(min_keep))\n",
    "\n",
    "    kept_total = 0\n",
    "    seen_total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        B = y.size(0)\n",
    "        seen_total += B\n",
    "\n",
    "        # how many to keep this batch\n",
    "        keep_k = max(min_keep, B - int(B * drop_frac))\n",
    "        keep_k = min(keep_k, B)  # safety\n",
    "\n",
    "        # pick random indices to keep\n",
    "        idx = torch.randperm(B, device=x.device)[:keep_k]\n",
    "        keep_mask = torch.zeros(B, dtype=torch.bool, device=x.device)\n",
    "        keep_mask[idx] = True\n",
    "\n",
    "        x_keep = x[keep_mask]\n",
    "        y_keep = y[keep_mask]\n",
    "        kept_total += y_keep.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_keep)\n",
    "        loss = loss_fn(logits, y_keep)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * y_keep.size(0)\n",
    "\n",
    "    avg_loss = running_loss / max(1, kept_total)\n",
    "    acceptance_rate = kept_total / max(1, seen_total)\n",
    "    return avg_loss, acceptance_rate\n",
    "\n",
    "# -------------------------\n",
    "# Main comparison runner\n",
    "# -------------------------\n",
    "def run_downstream_comparison(\n",
    "    trained_data_rater: nn.Module,\n",
    "    dataset_handler,   # your MNISTDataRaterDataset() instance\n",
    "    config: DownstreamConfig,\n",
    "    corruption_config=None  # if your dataset handler needs it for get_loaders\n",
    "):\n",
    "    set_seed(config.seed)\n",
    "\n",
    "    # Build loaders\n",
    "    train_loader, val_loader, test_loader = dataset_handler.get_loaders(\n",
    "        config.batch_size,\n",
    "        config.train_split_ratio,\n",
    "        corruption_config if corruption_config is not None else DataCorruptionConfig()\n",
    "    )\n",
    "\n",
    "    device = config.device\n",
    "    trained_data_rater = trained_data_rater.to(device).eval()\n",
    "\n",
    "    # Initialize three identical CNNs (same init => fair comparison)\n",
    "    base_init = construct_model('ToyCNN').to(device)\n",
    "    base_init.eval()\n",
    "    init_state = base_init.state_dict()\n",
    "\n",
    "    baseline_model = construct_model('ToyCNN').to(device)\n",
    "    filtered_model = construct_model('ToyCNN').to(device)\n",
    "    randomdrop_model = construct_model('ToyCNN').to(device)\n",
    "    baseline_model.load_state_dict(init_state)\n",
    "    filtered_model.load_state_dict(init_state)\n",
    "    randomdrop_model.load_state_dict(init_state)\n",
    "\n",
    "    # Optimizers\n",
    "    opt_base = optim.Adam(baseline_model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    opt_filt = optim.Adam(filtered_model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "    opt_rand = optim.Adam(randomdrop_model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    # -------- Baseline --------\n",
    "    print(f\"\\n=== Baseline training (no dropping) for {config.epochs} epochs ===\")\n",
    "    for ep in range(1, config.epochs + 1):\n",
    "        train_loss = train_one_epoch_baseline(baseline_model, train_loader, opt_base, device)\n",
    "        val_acc = evaluate(baseline_model, val_loader, device)\n",
    "        print(f\"[Baseline][Epoch {ep}/{config.epochs}] loss={train_loss:.4f}  val_acc={val_acc:.4f}\")\n",
    "    baseline_test_acc = evaluate(baseline_model, test_loader, device)\n",
    "    print(f\"[Baseline] Test Accuracy: {baseline_test_acc:.4f}\")\n",
    "\n",
    "    # -------- DataRater Filtered --------\n",
    "    print(f\"\\n=== Filtered training with DataRater (drop_frac={config.drop_frac:.2f}) for {config.epochs} epochs ===\")\n",
    "    for ep in range(1, config.epochs + 1):\n",
    "        train_loss, acceptance = train_one_epoch_filtered(\n",
    "            filtered_model, trained_data_rater, train_loader, opt_filt, device,\n",
    "            drop_frac=config.drop_frac, center=True, min_keep=8\n",
    "        )\n",
    "        val_acc = evaluate(filtered_model, val_loader, device)\n",
    "        print(f\"[Filtered][Epoch {ep}/{config.epochs}] loss={train_loss:.4f}  val_acc={val_acc:.4f}  acceptance={acceptance*100:.1f}%\")\n",
    "    filtered_test_acc = evaluate(filtered_model, test_loader, device)\n",
    "    print(f\"[Filtered] Test Accuracy: {filtered_test_acc:.4f}\")\n",
    "\n",
    "    # -------- Random Drop (control) --------\n",
    "    print(f\"\\n=== Random-drop training (drop_frac={config.drop_frac:.2f}) for {config.epochs} epochs ===\")\n",
    "    for ep in range(1, config.epochs + 1):\n",
    "        train_loss, acceptance = train_one_epoch_random_drop(\n",
    "            randomdrop_model, train_loader, opt_rand, device,\n",
    "            drop_frac=config.drop_frac, min_keep=8\n",
    "        )\n",
    "        val_acc = evaluate(randomdrop_model, val_loader, device)\n",
    "        print(f\"[RandomDrop][Epoch {ep}/{config.epochs}] loss={train_loss:.4f}  val_acc={val_acc:.4f}  acceptance={acceptance*100:.1f}%\")\n",
    "    randomdrop_test_acc = evaluate(randomdrop_model, test_loader, device)\n",
    "    print(f\"[RandomDrop] Test Accuracy: {randomdrop_test_acc:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Summary ===\")\n",
    "    print(f\"Baseline  Test Acc : {baseline_test_acc:.4f}\")\n",
    "    print(f\"Filtered  Test Acc : {filtered_test_acc:.4f}\")\n",
    "    print(f\"RandomDrop Test Acc: {randomdrop_test_acc:.4f}\")\n",
    "    return {\n",
    "        \"baseline_test_acc\": baseline_test_acc,\n",
    "        \"filtered_test_acc\": filtered_test_acc,\n",
    "        \"randomdrop_test_acc\": randomdrop_test_acc,\n",
    "    }\n",
    "\n",
    "def run_trials(\n",
    "    trained_data_rater,\n",
    "    dataset_handler,\n",
    "    base_config: DownstreamConfig,\n",
    "    corruption_config=None,\n",
    "    n_trials: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs n_trials of the downstream comparison, reseeding each time with base_config.seed + i.\n",
    "    Collects test accuracies and reports mean ± std.\n",
    "    \"\"\"\n",
    "    baseline_accs = []\n",
    "    filtered_accs = []\n",
    "    randomdrop_accs = []\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        cfg = DownstreamConfig(\n",
    "            device=base_config.device,\n",
    "            batch_size=base_config.batch_size,\n",
    "            train_split_ratio=base_config.train_split_ratio,\n",
    "            epochs=base_config.epochs,\n",
    "            lr=base_config.lr,\n",
    "            weight_decay=base_config.weight_decay,\n",
    "            keep_threshold=base_config.keep_threshold,\n",
    "            seed=base_config.seed + i,   # reseed per trial\n",
    "            drop_frac=base_config.drop_frac\n",
    "        )\n",
    "\n",
    "        print(f\"\\n========== Trial {i+1}/{n_trials} (seed={cfg.seed}) ==========\")\n",
    "        out = run_downstream_comparison(\n",
    "            trained_data_rater=trained_data_rater,\n",
    "            dataset_handler=dataset_handler,\n",
    "            config=cfg,\n",
    "            corruption_config=corruption_config\n",
    "        )\n",
    "        baseline_accs.append(out[\"baseline_test_acc\"])\n",
    "        filtered_accs.append(out[\"filtered_test_acc\"])\n",
    "        randomdrop_accs.append(out[\"randomdrop_test_acc\"])\n",
    "\n",
    "    baseline_accs = np.array(baseline_accs, dtype=float)\n",
    "    filtered_accs = np.array(filtered_accs, dtype=float)\n",
    "    randomdrop_accs = np.array(randomdrop_accs, dtype=float)\n",
    "\n",
    "    def _stat(a):\n",
    "        return float(a.mean()), float(a.std(ddof=1)) if len(a) > 1 else 0.0\n",
    "\n",
    "    b_mean, b_std = _stat(baseline_accs)\n",
    "    f_mean, f_std = _stat(filtered_accs)\n",
    "    r_mean, r_std = _stat(randomdrop_accs)\n",
    "\n",
    "    print(\"\\n=========== Final Summary over Trials ===========\")\n",
    "    print(f\"Baseline    : {b_mean:.4f} ± {b_std:.4f} (n={n_trials})\")\n",
    "    print(f\"Filtered    : {f_mean:.4f} ± {f_std:.4f} (n={n_trials})\")\n",
    "    print(f\"Random-Drop : {r_mean:.4f} ± {r_std:.4f} (n={n_trials})\")\n",
    "\n",
    "    return {\n",
    "        \"trials\": n_trials,\n",
    "        \"baseline_test_accs\": baseline_accs.tolist(),\n",
    "        \"filtered_test_accs\": filtered_accs.tolist(),\n",
    "        \"randomdrop_test_accs\": randomdrop_accs.tolist(),\n",
    "        \"summary\": {\n",
    "            \"baseline_mean\": b_mean, \"baseline_std\": b_std,\n",
    "            \"filtered_mean\": f_mean, \"filtered_std\": f_std,\n",
    "            \"randomdrop_mean\": r_mean, \"randomdrop_std\": r_std,\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3951978-0b27-4bbd-bf19-bcb9270691ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Trial 1/5 (seed=42) ==========\n",
      "Train set: 48000 images (probabilistically corrupted)\n",
      "Validation set: 12000 images (clean)\n",
      "Test set: 10000 images (clean)\n",
      "\n",
      "=== Baseline training (no dropping) for 1 epochs ===\n",
      "[Baseline][Epoch 1/1] loss=0.3752  val_acc=0.9665\n",
      "[Baseline] Test Accuracy: 0.9717\n",
      "\n",
      "=== Filtered training with DataRater (drop_frac=0.10) for 1 epochs ===\n",
      "[Filtered][Epoch 1/1] loss=0.3386  val_acc=0.9689  acceptance=90.6%\n",
      "[Filtered] Test Accuracy: 0.9736\n",
      "\n",
      "=== Random-drop training (drop_frac=0.10) for 1 epochs ===\n",
      "[RandomDrop][Epoch 1/1] loss=0.3743  val_acc=0.9634  acceptance=90.6%\n",
      "[RandomDrop] Test Accuracy: 0.9676\n",
      "\n",
      "=== Summary ===\n",
      "Baseline  Test Acc : 0.9717\n",
      "Filtered  Test Acc : 0.9736\n",
      "RandomDrop Test Acc: 0.9676\n",
      "\n",
      "========== Trial 2/5 (seed=43) ==========\n",
      "Train set: 48000 images (probabilistically corrupted)\n",
      "Validation set: 12000 images (clean)\n",
      "Test set: 10000 images (clean)\n",
      "\n",
      "=== Baseline training (no dropping) for 1 epochs ===\n",
      "[Baseline][Epoch 1/1] loss=0.3756  val_acc=0.9686\n",
      "[Baseline] Test Accuracy: 0.9728\n",
      "\n",
      "=== Filtered training with DataRater (drop_frac=0.10) for 1 epochs ===\n",
      "[Filtered][Epoch 1/1] loss=0.3208  val_acc=0.9655  acceptance=90.6%\n",
      "[Filtered] Test Accuracy: 0.9722\n",
      "\n",
      "=== Random-drop training (drop_frac=0.10) for 1 epochs ===\n",
      "[RandomDrop][Epoch 1/1] loss=0.3741  val_acc=0.9678  acceptance=90.6%\n",
      "[RandomDrop] Test Accuracy: 0.9731\n",
      "\n",
      "=== Summary ===\n",
      "Baseline  Test Acc : 0.9728\n",
      "Filtered  Test Acc : 0.9722\n",
      "RandomDrop Test Acc: 0.9731\n",
      "\n",
      "========== Trial 3/5 (seed=44) ==========\n",
      "Train set: 48000 images (probabilistically corrupted)\n",
      "Validation set: 12000 images (clean)\n",
      "Test set: 10000 images (clean)\n",
      "\n",
      "=== Baseline training (no dropping) for 1 epochs ===\n",
      "[Baseline][Epoch 1/1] loss=0.3759  val_acc=0.9688\n",
      "[Baseline] Test Accuracy: 0.9736\n",
      "\n",
      "=== Filtered training with DataRater (drop_frac=0.10) for 1 epochs ===\n",
      "[Filtered][Epoch 1/1] loss=0.3161  val_acc=0.9735  acceptance=90.6%\n",
      "[Filtered] Test Accuracy: 0.9765\n",
      "\n",
      "=== Random-drop training (drop_frac=0.10) for 1 epochs ===\n",
      "[RandomDrop][Epoch 1/1] loss=0.3724  val_acc=0.9704  acceptance=90.6%\n",
      "[RandomDrop] Test Accuracy: 0.9710\n",
      "\n",
      "=== Summary ===\n",
      "Baseline  Test Acc : 0.9736\n",
      "Filtered  Test Acc : 0.9765\n",
      "RandomDrop Test Acc: 0.9710\n",
      "\n",
      "========== Trial 4/5 (seed=45) ==========\n",
      "Train set: 48000 images (probabilistically corrupted)\n",
      "Validation set: 12000 images (clean)\n",
      "Test set: 10000 images (clean)\n",
      "\n",
      "=== Baseline training (no dropping) for 1 epochs ===\n",
      "[Baseline][Epoch 1/1] loss=0.3762  val_acc=0.9642\n",
      "[Baseline] Test Accuracy: 0.9697\n",
      "\n",
      "=== Filtered training with DataRater (drop_frac=0.10) for 1 epochs ===\n",
      "[Filtered][Epoch 1/1] loss=0.3464  val_acc=0.9658  acceptance=90.6%\n",
      "[Filtered] Test Accuracy: 0.9675\n",
      "\n",
      "=== Random-drop training (drop_frac=0.10) for 1 epochs ===\n",
      "[RandomDrop][Epoch 1/1] loss=0.4087  val_acc=0.9607  acceptance=90.6%\n",
      "[RandomDrop] Test Accuracy: 0.9654\n",
      "\n",
      "=== Summary ===\n",
      "Baseline  Test Acc : 0.9697\n",
      "Filtered  Test Acc : 0.9675\n",
      "RandomDrop Test Acc: 0.9654\n",
      "\n",
      "========== Trial 5/5 (seed=46) ==========\n",
      "Train set: 48000 images (probabilistically corrupted)\n",
      "Validation set: 12000 images (clean)\n",
      "Test set: 10000 images (clean)\n",
      "\n",
      "=== Baseline training (no dropping) for 1 epochs ===\n",
      "[Baseline][Epoch 1/1] loss=0.3678  val_acc=0.9643\n",
      "[Baseline] Test Accuracy: 0.9661\n",
      "\n",
      "=== Filtered training with DataRater (drop_frac=0.10) for 1 epochs ===\n",
      "[Filtered][Epoch 1/1] loss=0.3068  val_acc=0.9708  acceptance=90.6%\n",
      "[Filtered] Test Accuracy: 0.9760\n",
      "\n",
      "=== Random-drop training (drop_frac=0.10) for 1 epochs ===\n",
      "[RandomDrop][Epoch 1/1] loss=0.3729  val_acc=0.9691  acceptance=90.6%\n",
      "[RandomDrop] Test Accuracy: 0.9724\n",
      "\n",
      "=== Summary ===\n",
      "Baseline  Test Acc : 0.9661\n",
      "Filtered  Test Acc : 0.9760\n",
      "RandomDrop Test Acc: 0.9724\n",
      "\n",
      "=========== Final Summary over Trials ===========\n",
      "Baseline    : 0.9708 ± 0.0030 (n=5)\n",
      "Filtered    : 0.9732 ± 0.0036 (n=5)\n",
      "Random-Drop : 0.9699 ± 0.0033 (n=5)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def load_data_rater_from_checkpoint(checkpoint_path, device='cpu'):\n",
    "        model = construct_model('DataRater').to(device)\n",
    "        state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    checkpoint_path = \"mnist_20250920_1037_a11efc10/data_rater.pt\"\n",
    "    trained_data_rater = load_data_rater_from_checkpoint(checkpoint_path, DEVICE)\n",
    "\n",
    "    dataset_handler = MNISTDataRaterDataset()\n",
    "    corruption_cfg = DataCorruptionConfig()\n",
    "\n",
    "    base_cfg = DownstreamConfig(\n",
    "        device=DEVICE,\n",
    "        batch_size=128,\n",
    "        epochs=1,        # bump to 5–10 for a more stable comparison\n",
    "        lr=1e-3,\n",
    "        drop_frac=0.10\n",
    "    )\n",
    "\n",
    "    _ = run_trials(\n",
    "        trained_data_rater=trained_data_rater,\n",
    "        dataset_handler=dataset_handler,\n",
    "        base_config=base_cfg,\n",
    "        corruption_config=corruption_cfg,\n",
    "        n_trials=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9df3b-b49a-4430-90ac-86daae720b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
